"""
"""

import six
import logging

import scipy as sp
import scipy.special  # noqa

import numpy as np

__all__ = ['KDE']


class KDE(object):
    """

    Uses Fukunagaâ€™s method.
    """
    _BANDWIDTH_DEFAULT = 'scott'
    _SET_OFF_DIAGONAL = True

    def __init__(self, dataset, bandwidth=None, weights=None, neff=None, quiet=False):
        self.dataset = np.atleast_2d(dataset)
        if not self.dataset.size > 1:
            raise ValueError("`dataset` input should have multiple elements.")

        self._ndim, self._data_size = self.dataset.shape
        if weights is None:
            weights = np.ones(self.data_size)/self.data_size

        if weights is not None:
            if np.count_nonzero(weights) == 0 or np.any(~np.isfinite(weights) | (weights < 0)):
                raise ValueError("Invalid `weights` entries, all must be finite and > 0!")
            weights = np.atleast_1d(weights).astype(float)
            weights /= np.sum(weights)
            if weights.ndim != 1:
                raise ValueError("`weights` input should be one-dimensional.")
            if len(weights) != self.data_size:
                msg = "`weights` input should be of length N ({})".format(self.data_size)
                raise ValueError(msg)

        self._neff = neff
        self._weights = weights
        self._compute_covariance()
        self._quiet = quiet
        self.set_bandwidth(bandwidth=bandwidth)
        return

    def pdf(self, points, reflect=None):
        points = np.atleast_2d(points)

        ndim, nv = points.shape
        if ndim != self.ndim:
            if ndim == 1 and nv == self.ndim:
                # points was passed in as a row vector
                points = np.reshape(points, (self.ndim, 1))
                nv = 1
            else:
                msg = "points have dimension %s, dataset has dimension %s" % (ndim, self.ndim)
                raise ValueError(msg)

        result = np.zeros((nv,), dtype=float)

        '''
        # Compute Cholesky matrix from the covariance matrix, this matrix will transform the
        # dataset to one with a diagonal covariance matrix (i.e. independent variables)
        # note: `sp.linalg.cholesky` uses an *upper* matrix by default, and numpy uses *lower*
        whitening = sp.linalg.cholesky(self.bw_cov_inv)
        # Construct the 'whitened' (independent) dataset
        scaled_dataset = np.dot(whitening, self.dataset)
        # Construct the whitened sampling points
        scaled_points = np.dot(whitening, points)

        # Evaluate kernel at the target sample points

        # Determine the smaller dataset to loop over
        if nv >= self.data_size:
            # there are more points than data, so loop over data
            for i in range(self.data_size):
                diff = scaled_dataset[:, i, np.newaxis] - scaled_points
                energy = np.sum(diff * diff, axis=0) / 2.0
                result += self.weights[i]*np.exp(-energy)
        else:
            # loop over points
            for i in range(nv):
                diff = scaled_dataset - scaled_points[:, i, np.newaxis]
                energy = np.sum(diff * diff, axis=0) / 2.0
                result[i] = np.sum(np.exp(-energy)*self.weights, axis=0)
        '''

        whitening = sp.linalg.cholesky(self.bw_cov_inv)
        # Construct the 'whitened' (independent) dataset

        scaled_dataset = np.dot(whitening, self.dataset)
        # Construct the whitened sampling points
        scaled_points = np.dot(whitening, points)

        for i in range(self.data_size):
            diff = scaled_points - scaled_dataset[:, i, np.newaxis]
            energy = np.sum(diff * diff, axis=0) / 2.0
            result += self.weights[i]*np.exp(-energy)

        if reflect is not None:
            if np.shape(reflect) != (2,):
                raise ValueError("Reflect must have shape (2,) with lower and upper values!")

            for loc in reflect:
                if loc is None:
                    continue
                scaled_dataset = np.dot(whitening, self.dataset - loc)
                # Construct the whitened sampling points
                scaled_points = np.dot(whitening, points - loc)

                for i in range(self.data_size):
                    diff = scaled_points + scaled_dataset[:, i, np.newaxis]
                    energy = np.sum(diff * diff, axis=0) / 2.0
                    result += self.weights[i]*np.exp(-energy)

            reflect[0] = -np.inf if reflect[0] is None else reflect[0]
            reflect[1] = +np.inf if reflect[1] is None else reflect[1]
            idx = (points.squeeze() < reflect[0]) | (reflect[1] < points.squeeze())
            result[idx] = 0.0

        result = result / self.bw_norm

        return result

    def resample(self, size=None, keep=None):
        if size is None:
            size = int(self.neff)

        bw_cov = np.array(self.bw_cov)
        if keep is not None:
            keep = np.atleast_1d(keep)
            for pp in keep:
                bw_cov[pp, :] = 0.0
                bw_cov[:, pp] = 0.0

        # Draw from the smoothing kernel, here the `cov` includes the bandwidth
        norm = np.random.multivariate_normal(np.zeros(self.ndim), bw_cov, size=size).T
        # Draw randomly from the given data points, proportionally to their weights
        indices = np.random.choice(self.data_size, size=size, p=self.weights)
        means = self.dataset[:, indices]
        # Shift each re-drawn sample based on the kernel-samples
        samps = means + norm

        return samps

    def scott_factor(self, *args, **kwargs):
        return np.power(self.neff, -1./(self.ndim+4))

    def silverman_factor(self, *args, **kwargs):
        return np.power(self.neff*(self.ndim+2.0)/4.0, -1./(self.ndim+4))

    def set_bandwidth(self, bandwidth=None):
        ndim = self.ndim
        _bandwidth = bandwidth
        bw_white = np.zeros((ndim, ndim))

        if len(np.atleast_1d(bandwidth)) == 1:
            _bw, bw_type = self._compute_bandwidth(bandwidth)
            if self._SET_OFF_DIAGONAL:
                bw_white[...] = _bw
            else:
                idx = np.arange(ndim)
                bw_white[idx, idx] = _bw
        else:
            if np.shape(bandwidth) == (ndim,):
                # bw_method = 'diagonal'
                for ii in range(self.ndim):
                    bw_white[ii, ii] = self._compute_bandwidth(
                        bandwidth[ii], param=(ii, ii))[0]
                bw_type = 'diagonal'
            elif np.shape(bandwidth) == (ndim, ndim):
                for ii, jj in np.ndindex(ndim, ndim):
                    bw_white[ii, jj] = self._compute_bandwidth(
                        bandwidth[ii, jj], param=(ii, jj))[0]
                bw_type = 'matrix'
            else:
                raise ValueError("`bandwidth` have shape (1,), (N,) or (N,) for `N` dimensions!")

        bw_cov = self._data_cov * (bw_white ** 2)
        try:
            bw_cov_inv = np.linalg.inv(bw_cov)
        except np.linalg.LinAlgError:
            if not self._quiet:
                logging.warning("WARNING: singular `bw_cov` matrix, trying SVD...")
            bw_cov_inv = np.linalg.pinv(bw_cov)

        self.bw_white = bw_white
        self.bw_type = bw_type
        self._bandwidth = _bandwidth
        self.bw_cov = bw_cov
        self.bw_cov_inv = bw_cov_inv
        self.bw_norm = np.sqrt(np.linalg.det(2*np.pi*self.bw_cov))
        return

    def _compute_bandwidth(self, bandwidth, param=None):
        if bandwidth is None:
            bandwidth = self._BANDWIDTH_DEFAULT

        if isinstance(bandwidth, six.string_types):
            if bandwidth == 'scott':
                bw = self.scott_factor(param=param)
            elif bandwidth == 'silverman':
                bw = self.silverman_factor(param=param)
            else:
                msg = "Unrecognized bandwidth str specification '{}'!".format(bandwidth)
                raise ValueError(msg)

            bw_type = bandwidth

        elif np.isscalar(bandwidth):
            if np.isclose(bandwidth, 0.0):
                msg = "`bandwidth` '{}' for param '{}' cannot be zero!".format(bandwidth, param)
                raise ValueError(msg)

            bw = bandwidth
            bw_type = 'constant scalar'

        elif callable(bandwidth):
            bw = bandwidth(self, param=param)
            bw_type = 'function'

        else:
            raise ValueError("Unrecognized `bandwidth` '{}'!".format(bandwidth))

        return bw, bw_type

    def _compute_covariance(self):
        """Computes the covariance matrix for each Gaussian kernel using bandwidth_func().
        """

        # Cache covariance and inverse covariance of the data
        if not hasattr(self, '_data_cov'):
            cov = np.cov(self.dataset, rowvar=True, bias=False, aweights=self.weights)
            self._data_cov = np.atleast_2d(cov)
        # if not hasattr(self, '_data_inv_cov'):
        #     self._data_inv_cov = np.linalg.inv(self._data_cov)

        return

    @property
    def weights(self):
        try:
            if self._weights is None:
                raise AttributeError
            return self._weights
        except AttributeError:
            self._weights = np.ones(self.data_size)/self.data_size
            return self._weights

    @property
    def neff(self):
        try:
            if self._neff is None:
                raise AttributeError
            return self._neff
        except AttributeError:
            self._neff = 1.0 / np.sum(self.weights**2)
            return self._neff

    @property
    def ndim(self):
        try:
            if self._ndim is None:
                raise AttributeError
            return self._ndim
        except AttributeError:
            self._ndim, self._data_size = np.shape(self.dataset)
            return self._ndim

    @property
    def data_size(self):
        try:
            if self._data_size is None:
                raise AttributeError
            return self._data_size
        except AttributeError:
            self._ndim, self._data_size = np.shape(self.dataset)
            return self._data_size
